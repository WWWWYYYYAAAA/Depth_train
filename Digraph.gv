digraph {
	graph [size="174.75,174.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1708031336160 [label="
 (1, 1, 200, 200)" fillcolor=darkolivegreen1]
	1708031319616 [label=ConvolutionBackward0]
	1708031319136 -> 1708031319616
	1708031319136 [label=ReluBackward0]
	1708031319184 -> 1708031319136
	1708031319184 [label=ConvolutionBackward0]
	1708031318560 -> 1708031319184
	1708031318560 [label=UpsampleBilinear2DBackward0]
	1708031318272 -> 1708031318560
	1708031318272 [label=ReluBackward0]
	1708031318080 -> 1708031318272
	1708031318080 [label=AddBackward0]
	1708031317888 -> 1708031318080
	1708031317888 [label=MulBackward0]
	1708031317552 -> 1708031317888
	1708031317552 [label=NativeBatchNormBackward0]
	1708031317408 -> 1708031317552
	1708031317408 [label=ConvolutionBackward0]
	1708031317024 -> 1708031317408
	1708031317024 [label=ReluBackward0]
	1708031316688 -> 1708031317024
	1708031316688 [label=NativeBatchNormBackward0]
	1708031316592 -> 1708031316688
	1708031316592 [label=ConvolutionBackward0]
	1708031316208 -> 1708031316592
	1708031316208 [label=CatBackward0]
	1708031315968 -> 1708031316208
	1708031315968 [label=ConvolutionBackward0]
	1708031315728 -> 1708031315968
	1708031315728 [label=ReluBackward0]
	1708031315344 -> 1708031315728
	1708031315344 [label=AddBackward0]
	1708031315152 -> 1708031315344
	1708031315152 [label=MulBackward0]
	1708031314912 -> 1708031315152
	1708031314912 [label=NativeBatchNormBackward0]
	1708031314672 -> 1708031314912
	1708031314672 [label=ConvolutionBackward0]
	1708031314288 -> 1708031314672
	1708031314288 [label=ReluBackward0]
	1708031314048 -> 1708031314288
	1708031314048 [label=NativeBatchNormBackward0]
	1708031313952 -> 1708031314048
	1708031313952 [label=ConvolutionBackward0]
	1708031313568 -> 1708031313952
	1708031313568 [label=CatBackward0]
	1708031313232 -> 1708031313568
	1708031313232 [label=ConvolutionBackward0]
	1708031312992 -> 1708031313232
	1708031312992 [label=ReluBackward0]
	1708031312272 -> 1708031312992
	1708031312272 [label=AddBackward0]
	1708031132816 -> 1708031312272
	1708031132816 [label=MulBackward0]
	1708031133056 -> 1708031132816
	1708031133056 [label=NativeBatchNormBackward0]
	1708031135456 -> 1708031133056
	1708031135456 [label=ConvolutionBackward0]
	1708031135264 -> 1708031135456
	1708031135264 [label=ReluBackward0]
	1708031147648 -> 1708031135264
	1708031147648 [label=NativeBatchNormBackward0]
	1708031147072 -> 1708031147648
	1708031147072 [label=ConvolutionBackward0]
	1708031146112 -> 1708031147072
	1708031146112 [label=CatBackward0]
	1708031144768 -> 1708031146112
	1708031144768 [label=UpsampleBilinear2DBackward0]
	1708031144096 -> 1708031144768
	1708031144096 [label=ConvolutionBackward0]
	1708031143520 -> 1708031144096
	1708031143520 [label=ReluBackward0]
	1708031142224 -> 1708031143520
	1708031142224 [label=AddBackward0]
	1708031141600 -> 1708031142224
	1708031141600 [label=MulBackward0]
	1708031140784 -> 1708031141600
	1708031140784 [label=NativeBatchNormBackward0]
	1708031140304 -> 1708031140784
	1708031140304 [label=ConvolutionBackward0]
	1708031139488 -> 1708031140304
	1708031139488 [label=ReluBackward0]
	1708031139152 -> 1708031139488
	1708031139152 [label=NativeBatchNormBackward0]
	1708031138816 -> 1708031139152
	1708031138816 [label=ConvolutionBackward0]
	1708031138192 -> 1708031138816
	1708031138192 [label=CatBackward0]
	1708031137616 -> 1708031138192
	1708031137616 [label=UpsampleBilinear2DBackward0]
	1708031136368 -> 1708031137616
	1708031136368 [label=ConvolutionBackward0]
	1708031135168 -> 1708031136368
	1708031135168 [label=ReluBackward0]
	1708031134352 -> 1708031135168
	1708031134352 [label=NativeBatchNormBackward0]
	1708031133152 -> 1708031134352
	1708031133152 [label=ConvolutionBackward0]
	1708031136032 -> 1708031133152
	1708031136032 [label=ReluBackward0]
	1708031405472 -> 1708031136032
	1708031405472 [label=NativeBatchNormBackward0]
	1708031405328 -> 1708031405472
	1708031405328 [label=ConvolutionBackward0]
	1708031404944 -> 1708031405328
	1708031404944 [label=ReluBackward0]
	1708031404704 -> 1708031404944
	1708031404704 [label=NativeBatchNormBackward0]
	1708031404512 -> 1708031404704
	1708031404512 [label=ConvolutionBackward0]
	1708031404128 -> 1708031404512
	1708031404128 [label=ReluBackward0]
	1708031403888 -> 1708031404128
	1708031403888 [label=AddBackward0]
	1708031403840 -> 1708031403888
	1708031403840 [label=NativeBatchNormBackward0]
	1708031403408 -> 1708031403840
	1708031403408 [label=ConvolutionBackward0]
	1708031403024 -> 1708031403408
	1708031403024 [label=ReluBackward0]
	1708031402880 -> 1708031403024
	1708031402880 [label=NativeBatchNormBackward0]
	1708031402688 -> 1708031402880
	1708031402688 [label=ConvolutionBackward0]
	1708031403744 -> 1708031402688
	1708031403744 [label=ReluBackward0]
	1708031402112 -> 1708031403744
	1708031402112 [label=AddBackward0]
	1708031402016 -> 1708031402112
	1708031402016 [label=NativeBatchNormBackward0]
	1708031401680 -> 1708031402016
	1708031401680 [label=ConvolutionBackward0]
	1708031401296 -> 1708031401680
	1708031401296 [label=ReluBackward0]
	1708031401152 -> 1708031401296
	1708031401152 [label=NativeBatchNormBackward0]
	1708031400960 -> 1708031401152
	1708031400960 [label=ConvolutionBackward0]
	1708031402064 -> 1708031400960
	1708031402064 [label=ReluBackward0]
	1708031400384 -> 1708031402064
	1708031400384 [label=AddBackward0]
	1708031400288 -> 1708031400384
	1708031400288 [label=NativeBatchNormBackward0]
	1708031399952 -> 1708031400288
	1708031399952 [label=ConvolutionBackward0]
	1708031399568 -> 1708031399952
	1708031399568 [label=ReluBackward0]
	1708031399424 -> 1708031399568
	1708031399424 [label=NativeBatchNormBackward0]
	1708031399232 -> 1708031399424
	1708031399232 [label=ConvolutionBackward0]
	1708031137712 -> 1708031399232
	1708031137712 [label=ReluBackward0]
	1708031398656 -> 1708031137712
	1708031398656 [label=AddBackward0]
	1708031398560 -> 1708031398656
	1708031398560 [label=NativeBatchNormBackward0]
	1708031398224 -> 1708031398560
	1708031398224 [label=ConvolutionBackward0]
	1708031397840 -> 1708031398224
	1708031397840 [label=ReluBackward0]
	1708031397696 -> 1708031397840
	1708031397696 [label=NativeBatchNormBackward0]
	1708031397504 -> 1708031397696
	1708031397504 [label=ConvolutionBackward0]
	1708031398608 -> 1708031397504
	1708031398608 [label=ReluBackward0]
	1708031396928 -> 1708031398608
	1708031396928 [label=AddBackward0]
	1708031396832 -> 1708031396928
	1708031396832 [label=NativeBatchNormBackward0]
	1708031396496 -> 1708031396832
	1708031396496 [label=ConvolutionBackward0]
	1708031396112 -> 1708031396496
	1708031396112 [label=ReluBackward0]
	1708031395968 -> 1708031396112
	1708031395968 [label=NativeBatchNormBackward0]
	1708031395776 -> 1708031395968
	1708031395776 [label=ConvolutionBackward0]
	1708031396880 -> 1708031395776
	1708031396880 [label=ReluBackward0]
	1708031395200 -> 1708031396880
	1708031395200 [label=AddBackward0]
	1708031395104 -> 1708031395200
	1708031395104 [label=NativeBatchNormBackward0]
	1708031394768 -> 1708031395104
	1708031394768 [label=ConvolutionBackward0]
	1708031394384 -> 1708031394768
	1708031394384 [label=ReluBackward0]
	1708031394240 -> 1708031394384
	1708031394240 [label=NativeBatchNormBackward0]
	1708031394048 -> 1708031394240
	1708031394048 [label=ConvolutionBackward0]
	1708031395152 -> 1708031394048
	1708031395152 [label=ReluBackward0]
	1708031405904 -> 1708031395152
	1708031405904 [label=AddBackward0]
	1708031406000 -> 1708031405904
	1708031406000 [label=NativeBatchNormBackward0]
	1708031406144 -> 1708031406000
	1708031406144 [label=ConvolutionBackward0]
	1708031406336 -> 1708031406144
	1708031406336 [label=ReluBackward0]
	1708031406480 -> 1708031406336
	1708031406480 [label=NativeBatchNormBackward0]
	1708031406576 -> 1708031406480
	1708031406576 [label=ConvolutionBackward0]
	1708031405952 -> 1708031406576
	1708031405952 [label=ReluBackward0]
	1708031406864 -> 1708031405952
	1708031406864 [label=AddBackward0]
	1708031406960 -> 1708031406864
	1708031406960 [label=NativeBatchNormBackward0]
	1708031407104 -> 1708031406960
	1708031407104 [label=ConvolutionBackward0]
	1708031407296 -> 1708031407104
	1708031407296 [label=ReluBackward0]
	1708031407440 -> 1708031407296
	1708031407440 [label=NativeBatchNormBackward0]
	1708031407536 -> 1708031407440
	1708031407536 [label=ConvolutionBackward0]
	1708031406912 -> 1708031407536
	1708031406912 [label=ReluBackward0]
	1708031407824 -> 1708031406912
	1708031407824 [label=AddBackward0]
	1708031407920 -> 1708031407824
	1708031407920 [label=NativeBatchNormBackward0]
	1708031408064 -> 1708031407920
	1708031408064 [label=ConvolutionBackward0]
	1708031408256 -> 1708031408064
	1708031408256 [label=ReluBackward0]
	1708031408400 -> 1708031408256
	1708031408400 [label=NativeBatchNormBackward0]
	1708031408496 -> 1708031408400
	1708031408496 [label=ConvolutionBackward0]
	1708031145392 -> 1708031408496
	1708031145392 [label=ReluBackward0]
	1708031408784 -> 1708031145392
	1708031408784 [label=AddBackward0]
	1708031408880 -> 1708031408784
	1708031408880 [label=NativeBatchNormBackward0]
	1708031409024 -> 1708031408880
	1708031409024 [label=ConvolutionBackward0]
	1708031409216 -> 1708031409024
	1708031409216 [label=ReluBackward0]
	1708031409360 -> 1708031409216
	1708031409360 [label=NativeBatchNormBackward0]
	1708031409456 -> 1708031409360
	1708031409456 [label=ConvolutionBackward0]
	1708031408832 -> 1708031409456
	1708031408832 [label=ReluBackward0]
	1708031409744 -> 1708031408832
	1708031409744 [label=AddBackward0]
	1708031409840 -> 1708031409744
	1708031409840 [label=NativeBatchNormBackward0]
	1708031409984 -> 1708031409840
	1708031409984 [label=ConvolutionBackward0]
	1708031410128 -> 1708031409984
	1708031410128 [label=ReluBackward0]
	1708033818832 -> 1708031410128
	1708033818832 [label=NativeBatchNormBackward0]
	1708033818928 -> 1708033818832
	1708033818928 [label=ConvolutionBackward0]
	1708031409792 -> 1708033818928
	1708031409792 [label=ReluBackward0]
	1708031135600 -> 1708031409792
	1708031135600 [label=AddBackward0]
	1708033819024 -> 1708031135600
	1708033819024 [label=NativeBatchNormBackward0]
	1708033819360 -> 1708033819024
	1708033819360 [label=ConvolutionBackward0]
	1708033819552 -> 1708033819360
	1708033819552 [label=ReluBackward0]
	1708033819696 -> 1708033819552
	1708033819696 [label=NativeBatchNormBackward0]
	1708033819792 -> 1708033819696
	1708033819792 [label=ConvolutionBackward0]
	1708033819168 -> 1708033819792
	1708033819168 [label=ReluBackward0]
	1708033820080 -> 1708033819168
	1708033820080 [label=AddBackward0]
	1708033820176 -> 1708033820080
	1708033820176 [label=NativeBatchNormBackward0]
	1708033820320 -> 1708033820176
	1708033820320 [label=ConvolutionBackward0]
	1708033820512 -> 1708033820320
	1708033820512 [label=ReluBackward0]
	1708033820656 -> 1708033820512
	1708033820656 [label=NativeBatchNormBackward0]
	1708033820752 -> 1708033820656
	1708033820752 [label=ConvolutionBackward0]
	1708031313376 -> 1708033820752
	1708031313376 [label=ReluBackward0]
	1708033821040 -> 1708031313376
	1708033821040 [label=AddBackward0]
	1708033821136 -> 1708033821040
	1708033821136 [label=NativeBatchNormBackward0]
	1708033821280 -> 1708033821136
	1708033821280 [label=ConvolutionBackward0]
	1708033821472 -> 1708033821280
	1708033821472 [label=ReluBackward0]
	1708033821616 -> 1708033821472
	1708033821616 [label=NativeBatchNormBackward0]
	1708033821712 -> 1708033821616
	1708033821712 [label=ConvolutionBackward0]
	1708033821088 -> 1708033821712
	1708033821088 [label=ReluBackward0]
	1708033822000 -> 1708033821088
	1708033822000 [label=AddBackward0]
	1708033822096 -> 1708033822000
	1708033822096 [label=NativeBatchNormBackward0]
	1708033822240 -> 1708033822096
	1708033822240 [label=ConvolutionBackward0]
	1708033822432 -> 1708033822240
	1708033822432 [label=ReluBackward0]
	1708033822576 -> 1708033822432
	1708033822576 [label=NativeBatchNormBackward0]
	1708033822672 -> 1708033822576
	1708033822672 [label=ConvolutionBackward0]
	1708033822048 -> 1708033822672
	1708033822048 [label=ReluBackward0]
	1708033822960 -> 1708033822048
	1708033822960 [label=AddBackward0]
	1708033823056 -> 1708033822960
	1708033823056 [label=NativeBatchNormBackward0]
	1708033823200 -> 1708033823056
	1708033823200 [label=ConvolutionBackward0]
	1708033823392 -> 1708033823200
	1708033823392 [label=ReluBackward0]
	1708033823536 -> 1708033823392
	1708033823536 [label=NativeBatchNormBackward0]
	1708033823632 -> 1708033823536
	1708033823632 [label=ConvolutionBackward0]
	1708033823008 -> 1708033823632
	1708033823008 [label=MaxPool2DWithIndicesBackward0]
	1708033823920 -> 1708033823008
	1708033823920 [label=ReluBackward0]
	1708033824016 -> 1708033823920
	1708033824016 [label=NativeBatchNormBackward0]
	1708033824112 -> 1708033824016
	1708033824112 [label=ConvolutionBackward0]
	1708033824304 -> 1708033824112
	1708019251008 [label="encoder0.0.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1708019251008 -> 1708033824304
	1708033824304 [label=AccumulateGrad]
	1708033824064 -> 1708033824016
	1707991918496 [label="encoder0.1.weight
 (64)" fillcolor=lightblue]
	1707991918496 -> 1708033824064
	1708033824064 [label=AccumulateGrad]
	1708033823728 -> 1708033824016
	1707894488128 [label="encoder0.1.bias
 (64)" fillcolor=lightblue]
	1707894488128 -> 1708033823728
	1708033823728 [label=AccumulateGrad]
	1708033823824 -> 1708033823632
	1707894487488 [label="encoder1.1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1707894487488 -> 1708033823824
	1708033823824 [label=AccumulateGrad]
	1708033823584 -> 1708033823536
	1707894487568 [label="encoder1.1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1707894487568 -> 1708033823584
	1708033823584 [label=AccumulateGrad]
	1708033823440 -> 1708033823536
	1707894487408 [label="encoder1.1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1707894487408 -> 1708033823440
	1708033823440 [label=AccumulateGrad]
	1708033823344 -> 1708033823200
	1707894486928 [label="encoder1.1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1707894486928 -> 1708033823344
	1708033823344 [label=AccumulateGrad]
	1708033823152 -> 1708033823056
	1707894487008 [label="encoder1.1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1707894487008 -> 1708033823152
	1708033823152 [label=AccumulateGrad]
	1708033823104 -> 1708033823056
	1707894486848 [label="encoder1.1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1707894486848 -> 1708033823104
	1708033823104 [label=AccumulateGrad]
	1708033823008 -> 1708033822960
	1708033822864 -> 1708033822672
	1707944039568 [label="encoder1.1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1707944039568 -> 1708033822864
	1708033822864 [label=AccumulateGrad]
	1708033822624 -> 1708033822576
	1707944039488 [label="encoder1.1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1707944039488 -> 1708033822624
	1708033822624 [label=AccumulateGrad]
	1708033822480 -> 1708033822576
	1707944038768 [label="encoder1.1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1707944038768 -> 1708033822480
	1708033822480 [label=AccumulateGrad]
	1708033822384 -> 1708033822240
	1707894487808 [label="encoder1.1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1707894487808 -> 1708033822384
	1708033822384 [label=AccumulateGrad]
	1708033822192 -> 1708033822096
	1708030695024 [label="encoder1.1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1708030695024 -> 1708033822192
	1708033822192 [label=AccumulateGrad]
	1708033822144 -> 1708033822096
	1707894486608 [label="encoder1.1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1707894486608 -> 1708033822144
	1708033822144 [label=AccumulateGrad]
	1708033822048 -> 1708033822000
	1708033821904 -> 1708033821712
	1707894486048 [label="encoder1.1.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1707894486048 -> 1708033821904
	1708033821904 [label=AccumulateGrad]
	1708033821664 -> 1708033821616
	1707894486128 [label="encoder1.1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1707894486128 -> 1708033821664
	1708033821664 [label=AccumulateGrad]
	1708033821520 -> 1708033821616
	1707894475808 [label="encoder1.1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1707894475808 -> 1708033821520
	1708033821520 [label=AccumulateGrad]
	1708033821424 -> 1708033821280
	1707894485168 [label="encoder1.1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1707894485168 -> 1708033821424
	1708033821424 [label=AccumulateGrad]
	1708033821232 -> 1708033821136
	1707894485248 [label="encoder1.1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1707894485248 -> 1708033821232
	1708033821232 [label=AccumulateGrad]
	1708033821184 -> 1708033821136
	1707894485088 [label="encoder1.1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1707894485088 -> 1708033821184
	1708033821184 [label=AccumulateGrad]
	1708033821088 -> 1708033821040
	1708033820944 -> 1708033820752
	1707894522800 [label="encoder2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1707894522800 -> 1708033820944
	1708033820944 [label=AccumulateGrad]
	1708033820704 -> 1708033820656
	1707894481728 [label="encoder2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1707894481728 -> 1708033820704
	1708033820704 [label=AccumulateGrad]
	1708033820560 -> 1708033820656
	1707894522720 [label="encoder2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1707894522720 -> 1708033820560
	1708033820560 [label=AccumulateGrad]
	1708033820464 -> 1708033820320
	1707894522640 [label="encoder2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1707894522640 -> 1708033820464
	1708033820464 [label=AccumulateGrad]
	1708033820272 -> 1708033820176
	1707894528880 [label="encoder2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1707894528880 -> 1708033820272
	1708033820272 [label=AccumulateGrad]
	1708033820224 -> 1708033820176
	1707894522560 [label="encoder2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1707894522560 -> 1708033820224
	1708033820224 [label=AccumulateGrad]
	1708033820128 -> 1708033820080
	1708033820128 [label=NativeBatchNormBackward0]
	1708033820896 -> 1708033820128
	1708033820896 [label=ConvolutionBackward0]
	1708031313376 -> 1708033820896
	1708033820848 -> 1708033820896
	1707894484768 [label="encoder2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1707894484768 -> 1708033820848
	1708033820848 [label=AccumulateGrad]
	1708033820416 -> 1708033820128
	1707894484688 [label="encoder2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1707894484688 -> 1708033820416
	1708033820416 [label=AccumulateGrad]
	1708033820368 -> 1708033820128
	1707894475408 [label="encoder2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1707894475408 -> 1708033820368
	1708033820368 [label=AccumulateGrad]
	1708033819984 -> 1708033819792
	1707894522400 [label="encoder2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1707894522400 -> 1708033819984
	1708033819984 [label=AccumulateGrad]
	1708033819744 -> 1708033819696
	1707894522480 [label="encoder2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1707894522480 -> 1708033819744
	1708033819744 [label=AccumulateGrad]
	1708033819600 -> 1708033819696
	1707894535520 [label="encoder2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1707894535520 -> 1708033819600
	1708033819600 [label=AccumulateGrad]
	1708033819504 -> 1708033819360
	1707894535360 [label="encoder2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1707894535360 -> 1708033819504
	1708033819504 [label=AccumulateGrad]
	1708033819312 -> 1708033819024
	1707894522240 [label="encoder2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1707894522240 -> 1708033819312
	1708033819312 [label=AccumulateGrad]
	1708033819264 -> 1708033819024
	1707894535280 [label="encoder2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1707894535280 -> 1708033819264
	1708033819264 [label=AccumulateGrad]
	1708033819168 -> 1708031135600
	1708033819120 -> 1708033818928
	1707894528320 [label="encoder2.2.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1707894528320 -> 1708033819120
	1708033819120 [label=AccumulateGrad]
	1708033818880 -> 1708033818832
	1707894535120 [label="encoder2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1707894535120 -> 1708033818880
	1708033818880 [label=AccumulateGrad]
	1708033818736 -> 1708033818832
	1707894528240 [label="encoder2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1707894528240 -> 1708033818736
	1708033818736 [label=AccumulateGrad]
	1708031410080 -> 1708031409984
	1707894534880 [label="encoder2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1707894534880 -> 1708031410080
	1708031410080 [label=AccumulateGrad]
	1708031409936 -> 1708031409840
	1707894528080 [label="encoder2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1707894528080 -> 1708031409936
	1708031409936 [label=AccumulateGrad]
	1708031409888 -> 1708031409840
	1707894534800 [label="encoder2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1707894534800 -> 1708031409888
	1708031409888 [label=AccumulateGrad]
	1708031409792 -> 1708031409744
	1708031409648 -> 1708031409456
	1707894534720 [label="encoder2.3.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1707894534720 -> 1708031409648
	1708031409648 [label=AccumulateGrad]
	1708031409408 -> 1708031409360
	1707894527920 [label="encoder2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1707894527920 -> 1708031409408
	1708031409408 [label=AccumulateGrad]
	1708031409264 -> 1708031409360
	1707894534640 [label="encoder2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1707894534640 -> 1708031409264
	1708031409264 [label=AccumulateGrad]
	1708031409168 -> 1708031409024
	1707894534240 [label="encoder2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1707894534240 -> 1708031409168
	1708031409168 [label=AccumulateGrad]
	1708031408976 -> 1708031408880
	1707894527440 [label="encoder2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1707894527440 -> 1708031408976
	1708031408976 [label=AccumulateGrad]
	1708031408928 -> 1708031408880
	1707894534160 [label="encoder2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1707894534160 -> 1708031408928
	1708031408928 [label=AccumulateGrad]
	1708031408832 -> 1708031408784
	1708031408688 -> 1708031408496
	1707894533520 [label="encoder3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1707894533520 -> 1708031408688
	1708031408688 [label=AccumulateGrad]
	1708031408448 -> 1708031408400
	1707894533600 [label="encoder3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1707894533600 -> 1708031408448
	1708031408448 [label=AccumulateGrad]
	1708031408304 -> 1708031408400
	1707894526720 [label="encoder3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1707894526720 -> 1708031408304
	1708031408304 [label=AccumulateGrad]
	1708031408208 -> 1708031408064
	1707894533200 [label="encoder3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894533200 -> 1708031408208
	1708031408208 [label=AccumulateGrad]
	1708031408016 -> 1708031407920
	1707894533280 [label="encoder3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1707894533280 -> 1708031408016
	1708031408016 [label=AccumulateGrad]
	1708031407968 -> 1708031407920
	1707894526400 [label="encoder3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1707894526400 -> 1708031407968
	1708031407968 [label=AccumulateGrad]
	1708031407872 -> 1708031407824
	1708031407872 [label=NativeBatchNormBackward0]
	1708031408640 -> 1708031407872
	1708031408640 [label=ConvolutionBackward0]
	1708031145392 -> 1708031408640
	1708031408592 -> 1708031408640
	1707894527120 [label="encoder3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1707894527120 -> 1708031408592
	1708031408592 [label=AccumulateGrad]
	1708031408160 -> 1708031407872
	1707894533920 [label="encoder3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1707894533920 -> 1708031408160
	1708031408160 [label=AccumulateGrad]
	1708031408112 -> 1708031407872
	1707894533840 [label="encoder3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1707894533840 -> 1708031408112
	1708031408112 [label=AccumulateGrad]
	1708031407728 -> 1708031407536
	1707894532960 [label="encoder3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894532960 -> 1708031407728
	1708031407728 [label=AccumulateGrad]
	1708031407488 -> 1708031407440
	1707894526160 [label="encoder3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1707894526160 -> 1708031407488
	1708031407488 [label=AccumulateGrad]
	1708031407344 -> 1708031407440
	1707894532880 [label="encoder3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1707894532880 -> 1708031407344
	1708031407344 [label=AccumulateGrad]
	1708031407248 -> 1708031407104
	1707894532640 [label="encoder3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894532640 -> 1708031407248
	1708031407248 [label=AccumulateGrad]
	1708031407056 -> 1708031406960
	1707894525840 [label="encoder3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1707894525840 -> 1708031407056
	1708031407056 [label=AccumulateGrad]
	1708031407008 -> 1708031406960
	1707894525760 [label="encoder3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1707894525760 -> 1708031407008
	1708031407008 [label=AccumulateGrad]
	1708031406912 -> 1708031406864
	1708031406768 -> 1708031406576
	1707894532160 [label="encoder3.2.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894532160 -> 1708031406768
	1708031406768 [label=AccumulateGrad]
	1708031406528 -> 1708031406480
	1707894532240 [label="encoder3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1707894532240 -> 1708031406528
	1708031406528 [label=AccumulateGrad]
	1708031406384 -> 1708031406480
	1707894525600 [label="encoder3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1707894525600 -> 1708031406384
	1708031406384 [label=AccumulateGrad]
	1708031406288 -> 1708031406144
	1707894532000 [label="encoder3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894532000 -> 1708031406288
	1708031406288 [label=AccumulateGrad]
	1708031406096 -> 1708031406000
	1707894532080 [label="encoder3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1707894532080 -> 1708031406096
	1708031406096 [label=AccumulateGrad]
	1708031406048 -> 1708031406000
	1707894525120 [label="encoder3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1707894525120 -> 1708031406048
	1708031406048 [label=AccumulateGrad]
	1708031405952 -> 1708031405904
	1708031405808 -> 1708031394048
	1707894531680 [label="encoder3.3.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894531680 -> 1708031405808
	1708031405808 [label=AccumulateGrad]
	1708031394096 -> 1708031394240
	1707894531760 [label="encoder3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1707894531760 -> 1708031394096
	1708031394096 [label=AccumulateGrad]
	1708031394336 -> 1708031394240
	1707894524640 [label="encoder3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1707894524640 -> 1708031394336
	1708031394336 [label=AccumulateGrad]
	1708031394528 -> 1708031394768
	1707894531200 [label="encoder3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894531200 -> 1708031394528
	1708031394528 [label=AccumulateGrad]
	1708031394912 -> 1708031395104
	1707894531280 [label="encoder3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1707894531280 -> 1708031394912
	1708031394912 [label=AccumulateGrad]
	1708031394960 -> 1708031395104
	1707894524320 [label="encoder3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1707894524320 -> 1708031394960
	1708031394960 [label=AccumulateGrad]
	1708031395152 -> 1708031395200
	1708031395392 -> 1708031395776
	1707894530880 [label="encoder3.4.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894530880 -> 1708031395392
	1708031395392 [label=AccumulateGrad]
	1708031395824 -> 1708031395968
	1707894530960 [label="encoder3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1707894530960 -> 1708031395824
	1708031395824 [label=AccumulateGrad]
	1708031396064 -> 1708031395968
	1707894524000 [label="encoder3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1707894524000 -> 1708031396064
	1708031396064 [label=AccumulateGrad]
	1708031396256 -> 1708031396496
	1707894530560 [label="encoder3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894530560 -> 1708031396256
	1708031396256 [label=AccumulateGrad]
	1708031396640 -> 1708031396832
	1707894530640 [label="encoder3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1707894530640 -> 1708031396640
	1708031396640 [label=AccumulateGrad]
	1708031396688 -> 1708031396832
	1707894524800 [label="encoder3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1707894524800 -> 1708031396688
	1708031396688 [label=AccumulateGrad]
	1708031396880 -> 1708031396928
	1708031397120 -> 1708031397504
	1707894522880 [label="encoder3.5.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1707894522880 -> 1708031397120
	1708031397120 [label=AccumulateGrad]
	1708031397552 -> 1708031397696
	1707894522960 [label="encoder3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1707894522960 -> 1708031397552
	1708031397552 [label=AccumulateGrad]
	1708031397792 -> 1708031397696
	1707894536000 [label="encoder3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1707894536000 -> 1708031397792
	1708031397792 [label=AccumulateGrad]
	1708031397984 -> 1708031398224
	1708030844080 [label="encoder3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1708030844080 -> 1708031397984
	1708031397984 [label=AccumulateGrad]
	1708031398368 -> 1708031398560
	1708030844000 [label="encoder3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1708030844000 -> 1708031398368
	1708031398368 [label=AccumulateGrad]
	1708031398416 -> 1708031398560
	1708030844160 [label="encoder3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1708030844160 -> 1708031398416
	1708031398416 [label=AccumulateGrad]
	1708031398608 -> 1708031398656
	1708031398848 -> 1708031399232
	1708030845360 [label="encoder4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1708030845360 -> 1708031398848
	1708031398848 [label=AccumulateGrad]
	1708031399280 -> 1708031399424
	1708030845280 [label="encoder4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1708030845280 -> 1708031399280
	1708031399280 [label=AccumulateGrad]
	1708031399520 -> 1708031399424
	1708030845440 [label="encoder4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1708030845440 -> 1708031399520
	1708031399520 [label=AccumulateGrad]
	1708031399712 -> 1708031399952
	1708030846000 [label="encoder4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1708030846000 -> 1708031399712
	1708031399712 [label=AccumulateGrad]
	1708031400096 -> 1708031400288
	1708030845920 [label="encoder4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1708030845920 -> 1708031400096
	1708031400096 [label=AccumulateGrad]
	1708031400144 -> 1708031400288
	1708030846080 [label="encoder4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1708030846080 -> 1708031400144
	1708031400144 [label=AccumulateGrad]
	1708031400336 -> 1708031400384
	1708031400336 [label=NativeBatchNormBackward0]
	1708031398896 -> 1708031400336
	1708031398896 [label=ConvolutionBackward0]
	1708031137712 -> 1708031398896
	1708031399040 -> 1708031398896
	1708030844560 [label="encoder4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1708030844560 -> 1708031399040
	1708031399040 [label=AccumulateGrad]
	1708031399760 -> 1708031400336
	1708030844640 [label="encoder4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1708030844640 -> 1708031399760
	1708031399760 [label=AccumulateGrad]
	1708031399904 -> 1708031400336
	1708030844720 [label="encoder4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1708030844720 -> 1708031399904
	1708031399904 [label=AccumulateGrad]
	1708031400576 -> 1708031400960
	1708030846560 [label="encoder4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1708030846560 -> 1708031400576
	1708031400576 [label=AccumulateGrad]
	1708031401008 -> 1708031401152
	1708030846480 [label="encoder4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1708030846480 -> 1708031401008
	1708031401008 [label=AccumulateGrad]
	1708031401248 -> 1708031401152
	1708030846640 [label="encoder4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1708030846640 -> 1708031401248
	1708031401248 [label=AccumulateGrad]
	1708031401440 -> 1708031401680
	1708030847120 [label="encoder4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1708030847120 -> 1708031401440
	1708031401440 [label=AccumulateGrad]
	1708031401824 -> 1708031402016
	1708030847040 [label="encoder4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1708030847040 -> 1708031401824
	1708031401824 [label=AccumulateGrad]
	1708031401872 -> 1708031402016
	1708030847200 [label="encoder4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1708030847200 -> 1708031401872
	1708031401872 [label=AccumulateGrad]
	1708031402064 -> 1708031402112
	1708031402304 -> 1708031402688
	1708030847760 [label="encoder4.2.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1708030847760 -> 1708031402304
	1708031402304 [label=AccumulateGrad]
	1708031402736 -> 1708031402880
	1708030847680 [label="encoder4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1708030847680 -> 1708031402736
	1708031402736 [label=AccumulateGrad]
	1708031402976 -> 1708031402880
	1708030847840 [label="encoder4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1708030847840 -> 1708031402976
	1708031402976 [label=AccumulateGrad]
	1708031403168 -> 1708031403408
	1708030848320 [label="encoder4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1708030848320 -> 1708031403168
	1708031403168 [label=AccumulateGrad]
	1708031403552 -> 1708031403840
	1708030848240 [label="encoder4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1708030848240 -> 1708031403552
	1708031403552 [label=AccumulateGrad]
	1708031403600 -> 1708031403840
	1708030848400 [label="encoder4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1708030848400 -> 1708031403600
	1708031403600 [label=AccumulateGrad]
	1708031403744 -> 1708031403888
	1708031404272 -> 1708031404512
	1708031278784 [label="mid_dilated.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1708031278784 -> 1708031404272
	1708031404272 [label=AccumulateGrad]
	1708031404656 -> 1708031404704
	1708031278624 [label="mid_dilated.1.weight
 (512)" fillcolor=lightblue]
	1708031278624 -> 1708031404656
	1708031404656 [label=AccumulateGrad]
	1708031404800 -> 1708031404704
	1708031278704 [label="mid_dilated.1.bias
 (512)" fillcolor=lightblue]
	1708031278704 -> 1708031404800
	1708031404800 [label=AccumulateGrad]
	1708031404992 -> 1708031405328
	1708031277744 [label="mid_dilated.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1708031277744 -> 1708031404992
	1708031404992 [label=AccumulateGrad]
	1708031405376 -> 1708031405472
	1708031277664 [label="mid_dilated.4.weight
 (512)" fillcolor=lightblue]
	1708031277664 -> 1708031405376
	1708031405376 [label=AccumulateGrad]
	1708031405520 -> 1708031405472
	1708031277344 [label="mid_dilated.4.bias
 (512)" fillcolor=lightblue]
	1708031277344 -> 1708031405520
	1708031405520 [label=AccumulateGrad]
	1708031135888 -> 1708031133152
	1708031276464 [label="mid_dilated.6.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1708031276464 -> 1708031135888
	1708031135888 [label=AccumulateGrad]
	1708031132960 -> 1708031134352
	1708031276384 [label="mid_dilated.7.weight
 (512)" fillcolor=lightblue]
	1708031276384 -> 1708031132960
	1708031132960 [label=AccumulateGrad]
	1708031133440 -> 1708031134352
	1708031276544 [label="mid_dilated.7.bias
 (512)" fillcolor=lightblue]
	1708031276544 -> 1708031133440
	1708031133440 [label=AccumulateGrad]
	1708031136272 -> 1708031136368
	1708031275824 [label="up1.weight
 (512, 256, 2, 2)" fillcolor=lightblue]
	1708031275824 -> 1708031136272
	1708031136272 [label=AccumulateGrad]
	1708031137184 -> 1708031136368
	1708031275744 [label="up1.bias
 (256)" fillcolor=lightblue]
	1708031275744 -> 1708031137184
	1708031137184 [label=AccumulateGrad]
	1708031137712 -> 1708031138192
	1708031138384 -> 1708031138816
	1708031275344 [label="conv1.conv1.0.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	1708031275344 -> 1708031138384
	1708031138384 [label=AccumulateGrad]
	1708031139056 -> 1708031139152
	1708031275184 [label="conv1.conv1.1.weight
 (256)" fillcolor=lightblue]
	1708031275184 -> 1708031139056
	1708031139056 [label=AccumulateGrad]
	1708031139392 -> 1708031139152
	1708031275264 [label="conv1.conv1.1.bias
 (256)" fillcolor=lightblue]
	1708031275264 -> 1708031139392
	1708031139392 [label=AccumulateGrad]
	1708031139632 -> 1708031140304
	1708031274384 [label="conv1.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1708031274384 -> 1708031139632
	1708031139632 [label=AccumulateGrad]
	1708031140448 -> 1708031140784
	1708031274544 [label="conv1.conv2.1.weight
 (256)" fillcolor=lightblue]
	1708031274544 -> 1708031140448
	1708031140448 [label=AccumulateGrad]
	1708031140640 -> 1708031140784
	1708031274144 [label="conv1.conv2.1.bias
 (256)" fillcolor=lightblue]
	1708031274144 -> 1708031140640
	1708031140640 [label=AccumulateGrad]
	1708031141168 -> 1708031141600
	1708031141168 [label=SigmoidBackward0]
	1708031405184 -> 1708031141168
	1708031405184 [label=ConvolutionBackward0]
	1708031404320 -> 1708031405184
	1708031404320 [label=ReluBackward0]
	1708031404080 -> 1708031404320
	1708031404080 [label=ConvolutionBackward0]
	1708031403216 -> 1708031404080
	1708031403216 [label=MeanBackward1]
	1708031140784 -> 1708031403216
	1708031403936 -> 1708031404080
	1708031273344 [label="conv1.se.1.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	1708031273344 -> 1708031403936
	1708031403936 [label=AccumulateGrad]
	1708031404752 -> 1708031404080
	1708031273184 [label="conv1.se.1.bias
 (16)" fillcolor=lightblue]
	1708031273184 -> 1708031404752
	1708031404752 [label=AccumulateGrad]
	1708031405568 -> 1708031405184
	1708031272944 [label="conv1.se.3.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	1708031272944 -> 1708031405568
	1708031405568 [label=AccumulateGrad]
	1708031405664 -> 1708031405184
	1708031272784 [label="conv1.se.3.bias
 (256)" fillcolor=lightblue]
	1708031272784 -> 1708031405664
	1708031405664 [label=AccumulateGrad]
	1708031141744 -> 1708031142224
	1708031141744 [label=NativeBatchNormBackward0]
	1708031403360 -> 1708031141744
	1708031403360 [label=ConvolutionBackward0]
	1708031138192 -> 1708031403360
	1708031402544 -> 1708031403360
	1708031272464 [label="conv1.shortcut.0.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1708031272464 -> 1708031402544
	1708031402544 [label=AccumulateGrad]
	1708031404464 -> 1708031141744
	1708031272384 [label="conv1.shortcut.1.weight
 (256)" fillcolor=lightblue]
	1708031272384 -> 1708031404464
	1708031404464 [label=AccumulateGrad]
	1708031405136 -> 1708031141744
	1708031272544 [label="conv1.shortcut.1.bias
 (256)" fillcolor=lightblue]
	1708031272544 -> 1708031405136
	1708031405136 [label=AccumulateGrad]
	1708031143616 -> 1708031144096
	1708031271744 [label="up2.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	1708031271744 -> 1708031143616
	1708031143616 [label=AccumulateGrad]
	1708031144672 -> 1708031144096
	1708031271664 [label="up2.bias
 (128)" fillcolor=lightblue]
	1708031271664 -> 1708031144672
	1708031144672 [label=AccumulateGrad]
	1708031145392 -> 1708031146112
	1708031146208 -> 1708031147072
	1708031271184 [label="conv2.conv1.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	1708031271184 -> 1708031146208
	1708031146208 [label=AccumulateGrad]
	1708031147168 -> 1708031147648
	1708031271264 [label="conv2.conv1.1.weight
 (128)" fillcolor=lightblue]
	1708031271264 -> 1708031147168
	1708031147168 [label=AccumulateGrad]
	1708031135216 -> 1708031147648
	1708031270944 [label="conv2.conv1.1.bias
 (128)" fillcolor=lightblue]
	1708031270944 -> 1708031135216
	1708031135216 [label=AccumulateGrad]
	1708031135312 -> 1708031135456
	1708031270144 [label="conv2.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1708031270144 -> 1708031135312
	1708031135312 [label=AccumulateGrad]
	1708031135504 -> 1708031133056
	1708031269984 [label="conv2.conv2.1.weight
 (128)" fillcolor=lightblue]
	1708031269984 -> 1708031135504
	1708031135504 [label=AccumulateGrad]
	1708031135552 -> 1708031133056
	1708031270224 [label="conv2.conv2.1.bias
 (128)" fillcolor=lightblue]
	1708031270224 -> 1708031135552
	1708031135552 [label=AccumulateGrad]
	1708031133200 -> 1708031132816
	1708031133200 [label=SigmoidBackward0]
	1708031402160 -> 1708031133200
	1708031402160 [label=ConvolutionBackward0]
	1708031401632 -> 1708031402160
	1708031401632 [label=ReluBackward0]
	1708031401200 -> 1708031401632
	1708031401200 [label=ConvolutionBackward0]
	1708031400768 -> 1708031401200
	1708031400768 [label=MeanBackward1]
	1708031133056 -> 1708031400768
	1708031399088 -> 1708031401200
	1708031269264 [label="conv2.se.1.weight
 (8, 128, 1, 1)" fillcolor=lightblue]
	1708031269264 -> 1708031399088
	1708031399088 [label=AccumulateGrad]
	1708031400624 -> 1708031401200
	1708031268944 [label="conv2.se.1.bias
 (8)" fillcolor=lightblue]
	1708031268944 -> 1708031400624
	1708031400624 [label=AccumulateGrad]
	1708031402496 -> 1708031402160
	1708031268784 [label="conv2.se.3.weight
 (128, 8, 1, 1)" fillcolor=lightblue]
	1708031268784 -> 1708031402496
	1708031402496 [label=AccumulateGrad]
	1708031402352 -> 1708031402160
	1708031268864 [label="conv2.se.3.bias
 (128)" fillcolor=lightblue]
	1708031268864 -> 1708031402352
	1708031402352 [label=AccumulateGrad]
	1708031132864 -> 1708031312272
	1708031132864 [label=NativeBatchNormBackward0]
	1708031400816 -> 1708031132864
	1708031400816 [label=ConvolutionBackward0]
	1708031146112 -> 1708031400816
	1708031398176 -> 1708031400816
	1708031268384 [label="conv2.shortcut.0.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1708031268384 -> 1708031398176
	1708031398176 [label=AccumulateGrad]
	1708031401488 -> 1708031132864
	1708031268544 [label="conv2.shortcut.1.weight
 (128)" fillcolor=lightblue]
	1708031268544 -> 1708031401488
	1708031401488 [label=AccumulateGrad]
	1708031402928 -> 1708031132864
	1708031268144 [label="conv2.shortcut.1.bias
 (128)" fillcolor=lightblue]
	1708031268144 -> 1708031402928
	1708031402928 [label=AccumulateGrad]
	1708031313088 -> 1708031313232
	1708031267424 [label="up3.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	1708031267424 -> 1708031313088
	1708031313088 [label=AccumulateGrad]
	1708031313184 -> 1708031313232
	1708031267184 [label="up3.bias
 (64)" fillcolor=lightblue]
	1708031267184 -> 1708031313184
	1708031313184 [label=AccumulateGrad]
	1708031313376 -> 1708031313568
	1708031313616 -> 1708031313952
	1708031267104 [label="conv3.conv1.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	1708031267104 -> 1708031313616
	1708031313616 [label=AccumulateGrad]
	1708031314000 -> 1708031314048
	1708031266704 [label="conv3.conv1.1.weight
 (64)" fillcolor=lightblue]
	1708031266704 -> 1708031314000
	1708031314000 [label=AccumulateGrad]
	1708031314240 -> 1708031314048
	1708031266624 [label="conv3.conv1.1.bias
 (64)" fillcolor=lightblue]
	1708031266624 -> 1708031314240
	1708031314240 [label=AccumulateGrad]
	1708031314432 -> 1708031314672
	1708031265984 [label="conv3.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1708031265984 -> 1708031314432
	1708031314432 [label=AccumulateGrad]
	1708031314816 -> 1708031314912
	1708031265904 [label="conv3.conv2.1.weight
 (64)" fillcolor=lightblue]
	1708031265904 -> 1708031314816
	1708031314816 [label=AccumulateGrad]
	1708031314864 -> 1708031314912
	1708031265584 [label="conv3.conv2.1.bias
 (64)" fillcolor=lightblue]
	1708031265584 -> 1708031314864
	1708031314864 [label=AccumulateGrad]
	1708031314960 -> 1708031315152
	1708031314960 [label=SigmoidBackward0]
	1708031313760 -> 1708031314960
	1708031313760 [label=ConvolutionBackward0]
	1708031314096 -> 1708031313760
	1708031314096 [label=ReluBackward0]
	1708031397168 -> 1708031314096
	1708031397168 [label=ConvolutionBackward0]
	1708031398704 -> 1708031397168
	1708031398704 [label=MeanBackward1]
	1708031314912 -> 1708031398704
	1708031399472 -> 1708031397168
	1708031264704 [label="conv3.se.1.weight
 (4, 64, 1, 1)" fillcolor=lightblue]
	1708031264704 -> 1708031399472
	1708031399472 [label=AccumulateGrad]
	1708031398032 -> 1708031397168
	1708031264624 [label="conv3.se.1.bias
 (4)" fillcolor=lightblue]
	1708031264624 -> 1708031398032
	1708031398032 [label=AccumulateGrad]
	1708031313808 -> 1708031313760
	1708031264384 [label="conv3.se.3.weight
 (64, 4, 1, 1)" fillcolor=lightblue]
	1708031264384 -> 1708031313808
	1708031313808 [label=AccumulateGrad]
	1708031314624 -> 1708031313760
	1708031264224 [label="conv3.se.3.bias
 (64)" fillcolor=lightblue]
	1708031264224 -> 1708031314624
	1708031314624 [label=AccumulateGrad]
	1708031315296 -> 1708031315344
	1708031315296 [label=NativeBatchNormBackward0]
	1708031312368 -> 1708031315296
	1708031312368 [label=ConvolutionBackward0]
	1708031313568 -> 1708031312368
	1708031396448 -> 1708031312368
	1708031264464 [label="conv3.shortcut.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	1708031264464 -> 1708031396448
	1708031396448 [label=AccumulateGrad]
	1708031314480 -> 1708031315296
	1708031264064 [label="conv3.shortcut.1.weight
 (64)" fillcolor=lightblue]
	1708031264064 -> 1708031314480
	1708031314480 [label=AccumulateGrad]
	1708031315104 -> 1708031315296
	1708031263984 [label="conv3.shortcut.1.bias
 (64)" fillcolor=lightblue]
	1708031263984 -> 1708031315104
	1708031315104 [label=AccumulateGrad]
	1708031315776 -> 1708031315968
	1708031263104 [label="up4.weight
 (64, 32, 2, 2)" fillcolor=lightblue]
	1708031263104 -> 1708031315776
	1708031315776 [label=AccumulateGrad]
	1708031315824 -> 1708031315968
	1708031262784 [label="up4.bias
 (32)" fillcolor=lightblue]
	1708031262784 -> 1708031315824
	1708031315824 [label=AccumulateGrad]
	1708031316016 -> 1708031316208
	1708031316016 [label=UpsampleBilinear2DBackward0]
	1708033823920 -> 1708031316016
	1708031316352 -> 1708031316592
	1708031265744 [label="conv4.conv1.0.weight
 (64, 96, 3, 3)" fillcolor=lightblue]
	1708031265744 -> 1708031316352
	1708031316352 [label=AccumulateGrad]
	1708031316640 -> 1708031316688
	1708031266944 [label="conv4.conv1.1.weight
 (64)" fillcolor=lightblue]
	1708031266944 -> 1708031316640
	1708031316640 [label=AccumulateGrad]
	1708031316880 -> 1708031316688
	1708031269104 [label="conv4.conv1.1.bias
 (64)" fillcolor=lightblue]
	1708031269104 -> 1708031316880
	1708031316880 [label=AccumulateGrad]
	1708031317072 -> 1708031317408
	1708031271504 [label="conv4.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1708031271504 -> 1708031317072
	1708031317072 [label=AccumulateGrad]
	1708031317456 -> 1708031317552
	1708031272704 [label="conv4.conv2.1.weight
 (64)" fillcolor=lightblue]
	1708031272704 -> 1708031317456
	1708031317456 [label=AccumulateGrad]
	1708031317504 -> 1708031317552
	1708031273104 [label="conv4.conv2.1.bias
 (64)" fillcolor=lightblue]
	1708031273104 -> 1708031317504
	1708031317504 [label=AccumulateGrad]
	1708031317696 -> 1708031317888
	1708031317696 [label=SigmoidBackward0]
	1708031316400 -> 1708031317696
	1708031316400 [label=ConvolutionBackward0]
	1708031316832 -> 1708031316400
	1708031316832 [label=ReluBackward0]
	1708031315680 -> 1708031316832
	1708031315680 [label=ConvolutionBackward0]
	1708031315536 -> 1708031315680
	1708031315536 [label=MeanBackward1]
	1708031317552 -> 1708031315536
	1708031397312 -> 1708031315680
	1708031277104 [label="conv4.se.1.weight
 (4, 64, 1, 1)" fillcolor=lightblue]
	1708031277104 -> 1708031397312
	1708031397312 [label=AccumulateGrad]
	1708031400432 -> 1708031315680
	1708031277504 [label="conv4.se.1.bias
 (4)" fillcolor=lightblue]
	1708031277504 -> 1708031400432
	1708031400432 [label=AccumulateGrad]
	1708031316544 -> 1708031316400
	1708031278944 [label="conv4.se.3.weight
 (64, 4, 1, 1)" fillcolor=lightblue]
	1708031278944 -> 1708031316544
	1708031316544 [label=AccumulateGrad]
	1708031317264 -> 1708031316400
	1708031344400 [label="conv4.se.3.bias
 (64)" fillcolor=lightblue]
	1708031344400 -> 1708031317264
	1708031317264 [label=AccumulateGrad]
	1708031317936 -> 1708031318080
	1708031317936 [label=NativeBatchNormBackward0]
	1708031315488 -> 1708031317936
	1708031315488 [label=ConvolutionBackward0]
	1708031316208 -> 1708031315488
	1708031397360 -> 1708031315488
	1708031344320 [label="conv4.shortcut.0.weight
 (64, 96, 1, 1)" fillcolor=lightblue]
	1708031344320 -> 1708031397360
	1708031397360 [label=AccumulateGrad]
	1708031317216 -> 1708031317936
	1708031344000 [label="conv4.shortcut.1.weight
 (64)" fillcolor=lightblue]
	1708031344000 -> 1708031317216
	1708031317216 [label=AccumulateGrad]
	1708031317744 -> 1708031317936
	1708031343840 [label="conv4.shortcut.1.bias
 (64)" fillcolor=lightblue]
	1708031343840 -> 1708031317744
	1708031317744 [label=AccumulateGrad]
	1708031318608 -> 1708031319184
	1708031343200 [label="final_conv.0.weight
 (32, 64, 3, 3)" fillcolor=lightblue]
	1708031343200 -> 1708031318608
	1708031318608 [label=AccumulateGrad]
	1708031318800 -> 1708031319184
	1708031343040 [label="final_conv.0.bias
 (32)" fillcolor=lightblue]
	1708031343040 -> 1708031318800
	1708031318800 [label=AccumulateGrad]
	1708031318944 -> 1708031319616
	1708031342880 [label="final_conv.2.weight
 (1, 32, 1, 1)" fillcolor=lightblue]
	1708031342880 -> 1708031318944
	1708031318944 [label=AccumulateGrad]
	1708031318992 -> 1708031319616
	1708031342800 [label="final_conv.2.bias
 (1)" fillcolor=lightblue]
	1708031342800 -> 1708031318992
	1708031318992 [label=AccumulateGrad]
	1708031319616 -> 1708031336160
}
